# Environment settings
env:
  ENVIRONMENT: "development"  # Default environment (can be overridden by OS environment variables)
  LOG_LEVEL: "INFO"
  DEBUG: true

# File paths
file_paths:
  uploads_dir: "uploads"  # Directory for uploaded files (relative to project root)
  extracts_dir: "extracts"  # Directory for extracted data (relative to project root)
  examples_dir: "examples"  # Directory for example files (relative to project root)
  
# Example settings
examples:
  excel:
    sample_filename: "sample_data.xlsx"
    columns_to_extract: ["ID", "Product", "Price"]
    price_threshold: 15

# Medtronics pipeline settings
medtronics:
  input_file: "uploads/Medtronics - Asset Log Uploader.xlsx"
  output_dir: "outputs/pipeline/medtronic"
  db_name: "medtronics_assets.db"
  sheet_name: "Asset Data"
  columns_to_extract: ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X"]
  drop_na_columns: ["asset name"]  # Drop rows where these columns have NaN values

# Wichita Animal Shelter pipeline settings
wichita:
  input_file: "uploads/Asset_List Wichita Animal Shelter (1).csv"
  output_dir: "outputs/pipeline/wichita"
  db_name: "wichita_assets.db"
  columns_to_extract: []  # Empty list means use all columns
  drop_na_columns: ["Asset Name", "Asset Category Name"]  # Drop rows where these columns have NaN values

# Excel utilities settings
excel_utils:
  extraction:
    default:
      header_row: null  # Auto-detect
      drop_empty_rows: true
      clean_column_names: true
      strip_whitespace: true
      convert_dtypes: true
    asset_data:
      header_row: 6  # We know the header starts at row 7 (index 6)
    eq_ids:
      header_row: 0  # Header is in the first row
    cobie:
      header_row: 2  # Header is in the third row
    dropdowns:
      header_row: 0  # Header is in the first row
  
  validation:
    default:
      missing_values:
        threshold: 0.5  # Allow up to 50% missing values
        columns: null  # Check all columns
      duplicate_rows:
        subset: null  # Check all columns for duplicates
      data_types:
        date_columns: ["date", "scheduled delivery date", "actual on-site date"]
        numeric_columns: ["motor hp", "size"]
        string_columns: ["equipment name", "equipment tag id", "manufacturer"]
        boolean_columns: ["o&m received", "attic stock"]
    asset_data:
      value_ranges:
        motor hp:
          min: 0
          max: 1000
        size:
          min: 0
          max: null  # No upper limit
    eq_ids:
      required_columns: ["Lookup (for Uploader)", "Trade", "Precon System"]
  
  analysis:
    default:
      unique_values:
        max_unique_values: 20  # Maximum number of unique values to include in the result
      column_statistics:
        include_outliers: true
      text_analysis:
        include_pattern_analysis: true

# Database settings
databases:
  sqlite:
    url: "sqlite:///fca_dashboard.db"
  postgresql:
    url: "${POSTGRES_URL}"
    
# Pipeline settings
pipeline_settings:
  batch_size: 5000
  log_level: "${LOG_LEVEL}"  # Uses the environment variable from env section
  
# Table mappings
tables:
  equipment:
    mapping_type: "direct"
    column_mappings:
      tag: "Tag"
      name: "Name"
      description: "Description"
