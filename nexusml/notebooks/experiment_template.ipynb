{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NexusML Experiment Template\n",
        "\n",
        "This notebook provides a template for running experiments with the NexusML package. It demonstrates how to use the new architecture for equipment classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "First, let's import the necessary modules and set up the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Add the project root to the Python path if needed\n",
        "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
        "if project_root not in sys.path:\n",
        "    sys.path.append(project_root)\n",
        "\n",
        "# Import NexusML modules\n",
        "from nexusml.core.di.container import DIContainer\n",
        "from nexusml.core.pipeline.context import PipelineContext\n",
        "from nexusml.core.pipeline.factory import PipelineFactory\n",
        "from nexusml.core.pipeline.orchestrator import PipelineOrchestrator\n",
        "from nexusml.core.pipeline.registry import ComponentRegistry\n",
        "from nexusml.core.config.provider import ConfigurationProvider\n",
        "\n",
        "# Set up plotting\n",
        "%matplotlib inline\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_context('notebook')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Let's set up the configuration for our experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the configuration provider\n",
        "config_provider = ConfigurationProvider()\n",
        "\n",
        "# Get the configuration\n",
        "config = config_provider.config\n",
        "\n",
        "# Print configuration details\n",
        "print(f\"Configuration loaded from: {config_provider._load_config.__name__}\")\n",
        "print(f\"Feature Engineering Configuration: {len(config.feature_engineering.text_combinations)} text combinations, {len(config.feature_engineering.numeric_columns)} numeric columns\")\n",
        "print(f\"Classification Configuration: {len(config.classification.classification_targets)} classification targets\")\n",
        "print(f\"Data Configuration: {len(config.data.required_columns)} required columns\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading and Exploration\n",
        "\n",
        "Now, let's load the data and explore it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the path to the data file\n",
        "data_path = \"../examples/sample_data.xlsx\"\n",
        "\n",
        "# Load the data using pandas\n",
        "data = pd.read_excel(data_path)\n",
        "\n",
        "# Display the first few rows\n",
        "print(f\"Data shape: {data.shape}\")\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explore the data\n",
        "print(\"Data types:\")\n",
        "print(data.dtypes)\n",
        "\n",
        "print(\"\\nMissing values:\")\n",
        "print(data.isnull().sum())\n",
        "\n",
        "print(\"\\nSummary statistics:\")\n",
        "data.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pipeline Setup\n",
        "\n",
        "Let's set up the pipeline components for our experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import component implementations\n",
        "from nexusml.core.pipeline.components.data_loader import CSVDataLoader, ExcelDataLoader\n",
        "from nexusml.core.pipeline.components.data_preprocessor import StandardPreprocessor\n",
        "from nexusml.core.pipeline.components.feature_engineer import TextFeatureEngineer\n",
        "from nexusml.core.pipeline.components.model_builder import RandomForestModelBuilder\n",
        "from nexusml.core.pipeline.components.model_trainer import StandardModelTrainer\n",
        "from nexusml.core.pipeline.components.model_evaluator import StandardModelEvaluator\n",
        "from nexusml.core.pipeline.components.model_serializer import PickleModelSerializer\n",
        "from nexusml.core.pipeline.components.predictor import StandardPredictor\n",
        "\n",
        "# Create a registry and container\n",
        "registry = ComponentRegistry()\n",
        "container = DIContainer()\n",
        "\n",
        "# Register components\n",
        "registry.register(DataLoader, \"csv\", CSVDataLoader)\n",
        "registry.register(DataLoader, \"excel\", ExcelDataLoader)\n",
        "registry.register(DataPreprocessor, \"standard\", StandardPreprocessor)\n",
        "registry.register(FeatureEngineer, \"text\", TextFeatureEngineer)\n",
        "registry.register(ModelBuilder, \"random_forest\", RandomForestModelBuilder)\n",
        "registry.register(ModelTrainer, \"standard\", StandardModelTrainer)\n",
        "registry.register(ModelEvaluator, \"standard\", StandardModelEvaluator)\n",
        "registry.register(ModelSerializer, \"pickle\", PickleModelSerializer)\n",
        "registry.register(Predictor, \"standard\", StandardPredictor)\n",
        "\n",
        "# Set default implementations\n",
        "registry.set_default_implementation(DataLoader, \"excel\")\n",
        "registry.set_default_implementation(DataPreprocessor, \"standard\")\n",
        "registry.set_default_implementation(FeatureEngineer, \"text\")\n",
        "registry.set_default_implementation(ModelBuilder, \"random_forest\")\n",
        "registry.set_default_implementation(ModelTrainer, \"standard\")\n",
        "registry.set_default_implementation(ModelEvaluator, \"standard\")\n",
        "registry.set_default_implementation(ModelSerializer, \"pickle\")\n",
        "registry.set_default_implementation(Predictor, \"standard\")\n",
        "\n",
        "# Create a factory and orchestrator\n",
        "factory = PipelineFactory(registry, container)\n",
        "context = PipelineContext()\n",
        "orchestrator = PipelineOrchestrator(factory, context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Training\n",
        "\n",
        "Now, let's train a model using the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train a model\n",
        "try:\n",
        "    model, metrics = orchestrator.train_model(\n",
        "        data_path=data_path,\n",
        "        test_size=0.3,\n",
        "        random_state=42,\n",
        "        optimize_hyperparameters=True,\n",
        "        output_dir=\"../outputs/models\",\n",
        "        model_name=\"equipment_classifier_experiment\",\n",
        "    )\n",
        "    \n",
        "    print(\"Model training completed successfully\")\n",
        "    print(f\"Model saved to: {orchestrator.context.get('model_path')}\")\n",
        "    print(f\"Metadata saved to: {orchestrator.context.get('metadata_path')}\")\n",
        "    print(\"Metrics:\")\n",
        "    for key, value in metrics.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error training model: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Evaluation\n",
        "\n",
        "Let's evaluate the model in more detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "try:\n",
        "    results = orchestrator.evaluate(\n",
        "        model=model,\n",
        "        data_path=data_path,\n",
        "        output_path=\"../outputs/evaluation_results_experiment.json\",\n",
        "    )\n",
        "    \n",
        "    print(\"Evaluation completed successfully\")\n",
        "    print(f\"Evaluation results saved to: ../outputs/evaluation_results_experiment.json\")\n",
        "    print(\"Metrics:\")\n",
        "    for key, value in results[\"metrics\"].items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error evaluating model: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization\n",
        "\n",
        "Let's visualize the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the metrics\n",
        "try:\n",
        "    # Create a bar chart of the metrics\n",
        "    metrics_df = pd.DataFrame(list(metrics.items()), columns=['Metric', 'Value'])\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x='Metric', y='Value', data=metrics_df)\n",
        "    plt.title('Model Metrics')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Create a confusion matrix if available\n",
        "    if 'confusion_matrix' in results['analysis']:\n",
        "        cm = results['analysis']['confusion_matrix']\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('Actual')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "except Exception as e:\n",
        "    print(f\"Error visualizing results: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prediction\n",
        "\n",
        "Finally, let's make predictions on new data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample data for prediction\n",
        "prediction_data = pd.DataFrame({\n",
        "    \"equipment_tag\": [\"AHU-01\", \"CHW-01\", \"P-01\"],\n",
        "    \"manufacturer\": [\"Trane\", \"Carrier\", \"Armstrong\"],\n",
        "    \"model\": [\"M-1000\", \"C-2000\", \"A-3000\"],\n",
        "    \"description\": [\n",
        "        \"Air Handling Unit with cooling coil\",\n",
        "        \"Centrifugal Chiller for HVAC system\",\n",
        "        \"Centrifugal Pump for chilled water\",\n",
        "    ],\n",
        "})\n",
        "\n",
        "# Make predictions\n",
        "try:\n",
        "    predictions = orchestrator.predict(\n",
        "        model=model,\n",
        "        data=prediction_data,\n",
        "        output_path=\"../outputs/predictions_experiment.csv\",\n",
        "    )\n",
        "    \n",
        "    print(\"Predictions completed successfully\")\n",
        "    print(f\"Predictions saved to: {orchestrator.context.get('output_path')}\")\n",
        "    print(\"Sample predictions:\")\n",
        "    display(predictions)\n",
        "except Exception as e:\n",
        "    print(f\"Error making predictions: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we demonstrated how to use the NexusML package for equipment classification. We loaded data, trained a model, evaluated it, and made predictions using the new architecture.\n",
        "\n",
        "The new architecture provides a flexible, maintainable, and testable system for equipment classification. By following a modular design with clear interfaces, dependency injection, and a factory pattern, it makes it easy to create, configure, and extend the pipeline."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
